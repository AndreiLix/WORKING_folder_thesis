{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Circle shenanningans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "\n",
    "\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_GoalPos( proj, radius, center_circle, angle_increment= 1):\n",
    "\n",
    "    \"\"\"\n",
    "    A function that returns the goal position of the Ant. \n",
    "        The goal position is computed by getting the angle between the lines (center_circle, ant_proj_line) and (center_circle, positive_XAxis),\n",
    "            increasing the angle by angle_incremet, computing the sin and cos of the new angle, multiplying them by radius and adding center_circle.\n",
    "                The resulting coordinates are those of goal_pos - a step along the circle in the counterclockwise direction\n",
    " \n",
    "    Parameters:\n",
    "    -----------\n",
    "    proj: np array size (2,): the x, y coordinates of the proj_lineection of the Ant.\n",
    "    danger_zone: float between 0 and 1, default = 0.95.\n",
    "        This is a fast and dirty implementation, so I have to micromanage at parts. When the Ant dets close to the intersections of the circle and the x axis, \n",
    "            the y coordinate of goal_pos must change its sign.\n",
    "    angle_incremet: float, the increment(degrees) added to the new angle used to compute goal_pos\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    np array size (2,) = the x, y coordinates of the goal position\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    import math\n",
    "    import numpy as np\n",
    "\n",
    "    proj_line =  np.array([center_circle, proj ]).tolist()    # transforming np arrays to embedded lists, since np's array opperations perturb the algorithm\n",
    "    x_axis_line = np.array([ center_circle, [ center_circle[0] + radius, center_circle[1]] ] ).tolist()\n",
    "\n",
    "    try:\n",
    "        slope1 = (x_axis_line[1][1] - x_axis_line[0][1]) / (x_axis_line[1][0] - x_axis_line[0][0])\n",
    "    except ZeroDivisionError:\n",
    "        slope1 = (x_axis_line[1][1] - x_axis_line[0][1]) / 1e-8\n",
    "\n",
    "    try:\n",
    "        slope2 = (proj_line[1][1] - proj_line[0][1]) / (proj_line[1][0] - proj_line[0][0])\n",
    "    except ZeroDivisionError:\n",
    "        slope2 = (proj_line[1][1] - proj_line[0][1]) / 1e-8\n",
    "\n",
    "    # Calculate the angle between the two lines\n",
    "    angle_rad = math.atan2(slope2 - slope1, 1 + slope1 * slope2)\n",
    "\n",
    "    new_angle_rad = math.radians( math.degrees(angle_rad) + angle_increment )\n",
    "\n",
    "    new_sin, new_cos = math.sin(new_angle_rad), math.cos(new_angle_rad)\n",
    "\n",
    "    print(center_circle)\n",
    "    \n",
    "    print(np.array( [new_sin, new_cos] ) * radius)\n",
    "    goal_pos = ( np.array( [new_sin, new_cos] ) * radius ) \n",
    "\n",
    "    return goal_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 6]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2])\n",
    "b = np.array([3,4])\n",
    "print(a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10.0\n"
     ]
    }
   ],
   "source": [
    "print( -1.00000000e+01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  0]\n",
      "[ 1.00000021e-08 -1.00000000e+01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.00000021e-08, -1.00000000e+01])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center_circle = np.array([10, 0])\n",
    "proj =   np.array([10, 10])\n",
    "radius = 10\n",
    "\n",
    "get_GoalPos(proj= proj, center_circle= center_circle, radius= radius, angle_increment= 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8939966636005579\n"
     ]
    }
   ],
   "source": [
    "print(math.sin(90))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sb3 testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example basic usage from here:https://stable-baselines3.readthedocs.io/en/master/guide/examples.html\n",
    "\n",
    "\n",
    "import gym\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'CartPole-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m DQN(\u001b[39m\"\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m\"\u001b[39m, env \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCartPole-v1\u001b[39m\u001b[39m\"\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39m# Train the agent and display a progress bar\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, progress_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Save the agent\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mdqn_lunar\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/dqn/dqn.py:265\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    256\u001b[0m     \u001b[39mself\u001b[39m: SelfDQN,\n\u001b[1;32m    257\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    263\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfDQN:\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    266\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    267\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    268\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    269\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    270\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    271\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    272\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    314\u001b[0m     \u001b[39mself\u001b[39m: SelfOffPolicyAlgorithm,\n\u001b[1;32m    315\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    321\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfOffPolicyAlgorithm:\n\u001b[0;32m--> 323\u001b[0m     total_timesteps, callback \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_learn(\n\u001b[1;32m    324\u001b[0m         total_timesteps,\n\u001b[1;32m    325\u001b[0m         callback,\n\u001b[1;32m    326\u001b[0m         reset_num_timesteps,\n\u001b[1;32m    327\u001b[0m         tb_log_name,\n\u001b[1;32m    328\u001b[0m         progress_bar,\n\u001b[1;32m    329\u001b[0m     )\n\u001b[1;32m    331\u001b[0m     callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    333\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:305\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._setup_learn\u001b[0;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m     pos \u001b[39m=\u001b[39m (replay_buffer\u001b[39m.\u001b[39mpos \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m replay_buffer\u001b[39m.\u001b[39mbuffer_size\n\u001b[1;32m    303\u001b[0m     replay_buffer\u001b[39m.\u001b[39mdones[pos] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_setup_learn(\n\u001b[1;32m    306\u001b[0m     total_timesteps,\n\u001b[1;32m    307\u001b[0m     callback,\n\u001b[1;32m    308\u001b[0m     reset_num_timesteps,\n\u001b[1;32m    309\u001b[0m     tb_log_name,\n\u001b[1;32m    310\u001b[0m     progress_bar,\n\u001b[1;32m    311\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:408\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[0;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[39m# Avoid resetting the environment when calling ``.learn()`` consecutive times\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[39mif\u001b[39;00m reset_num_timesteps \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_obs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 408\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mreset()  \u001b[39m# pytype: disable=annotation-type-mismatch\u001b[39;00m\n\u001b[1;32m    409\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_episode_starts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mnum_envs,), dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[1;32m    410\u001b[0m     \u001b[39m# Retrieve unnormalized observation for saving into the buffer\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:75\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m     74\u001b[0m     obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs[env_idx]\u001b[39m.\u001b[39mreset()\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_obs(env_idx, obs)\n\u001b[1;32m     76\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obs_from_buf()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:105\u001b[0m, in \u001b[0;36mDummyVecEnv._save_obs\u001b[0;34m(self, env_idx, obs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys:\n\u001b[1;32m    104\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuf_obs[key][env_idx] \u001b[39m=\u001b[39m obs\n\u001b[1;32m    106\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_obs[key][env_idx] \u001b[39m=\u001b[39m obs[key]\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 1."
     ]
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "model = DQN(\"MlpPolicy\", env = \"CartPole-v1\", verbose=1)\n",
    "# Train the agent and display a progress bar\n",
    "model.learn(total_timesteps=3, progress_bar=True)\n",
    "# Save the agent\n",
    "model.save(\"dqn_lunar\")\n",
    "del model  # delete trained model to demonstrate loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()\n",
    "\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained agent\n",
    "# NOTE: if you have loading issue, you can pass `print_system_info=True`\n",
    "# to compare the system on which the model was trained vs the current one\n",
    "# model = DQN.load(\"dqn_lunar\", env=env, print_system_info=True)\n",
    "model = DQN.load(\"dqn_lunar\", env=env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent\n",
    "# NOTE: If you use wrappers with your environment that modify rewards,\n",
    "#       this will be reflected here. To evaluate with original rewards,\n",
    "#       wrap environment in a \"Monitor\" wrapper before other wrappers.\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enjoy trained agent\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
